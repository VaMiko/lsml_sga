{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CGHGHkqE4rJw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3duPlU1fjxPz"
   },
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jovyan\n",
      " * Starting OpenBSD Secure Shell server sshd\n",
      "start-stop-daemon: unable to set gid to 0 (Operation not permitted)\n",
      "   ...fail!\n",
      " * sshd is running\n",
      "Starting namenodes on [localhost]\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: namenode is running as process 165.  Stop it first and ensure /tmp/hadoop-jovyan-namenode.pid file is empty before retry.\n",
      "Starting datanodes\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: datanode is running as process 277.  Stop it first and ensure /tmp/hadoop-jovyan-datanode.pid file is empty before retry.\n",
      "Starting secondary namenodes [6b4ace4c98df]\n",
      "6b4ace4c98df: Warning: Permanently added '6b4ace4c98df' (ED25519) to the list of known hosts.\n",
      "6b4ace4c98df: secondarynamenode is running as process 493.  Stop it first and ensure /tmp/hadoop-jovyan-secondarynamenode.pid file is empty before retry.\n",
      "Starting resourcemanager\n",
      "resourcemanager is running as process 733.  Stop it first and ensure /tmp/hadoop-jovyan-resourcemanager.pid file is empty before retry.\n",
      "Starting nodemanagers\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: nodemanager is running as process 859.  Stop it first and ensure /tmp/hadoop-jovyan-nodemanager.pid file is empty before retry.\n",
      "WARNING: Use of this script to start the MR JobHistory daemon is deprecated.\n",
      "WARNING: Attempting to execute replacement \"mapred --daemon start\" instead.\n",
      "12176 sun.tools.jps.Jps -lm\n",
      "277 org.apache.hadoop.hdfs.server.datanode.DataNode\n",
      "165 org.apache.hadoop.hdfs.server.namenode.NameNode\n",
      "859 org.apache.hadoop.yarn.server.nodemanager.NodeManager\n",
      "493 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\n",
      "733 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\n",
      "Configured Capacity: 269490393088 (250.98 GB)\n",
      "Present Capacity: 243121041408 (226.42 GB)\n",
      "DFS Remaining: 243121012736 (226.42 GB)\n",
      "DFS Used: 28672 (28 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:9866 (localhost)\n",
      "Hostname: 6b4ace4c98df\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 269490393088 (250.98 GB)\n",
      "DFS Used: 28672 (28 KB)\n",
      "Non DFS Used: 12608679936 (11.74 GB)\n",
      "DFS Remaining: 243121012736 (226.42 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 90.22%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Tue May 09 16:49:07 UTC 2023\n",
      "Last Block Report: Tue May 09 14:36:48 UTC 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "chmod: changing permissions of '/home/jovyan/jupyter.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/nginx.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/error.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/access.log': Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "! /home/jovyan/start-hadoop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u566smRWkDOS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-05-09 16:49:13,481 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# connect, context, session\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCrxvDnx47f2",
    "tags": []
   },
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-tZacCcy49Lv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                Size     Used  Available  Use%\n",
      "hdfs://localhost:9000  251.0 G  291.6 M    225.9 G    0%\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1m8mBXzZ4-kB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwx---   - root   supergroup          0 2023-05-09 16:49 /tmp\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-05-09 16:49 /user\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `page_views_sample.csv': File exists\n",
      "put: `documents_topics.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "# were downloaded from chat in tg\n",
    "! hdfs dfs -put page_views_sample.csv\n",
    "! hdfs dfs -put documents_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "views = se.read.csv(\"page_views_sample.csv\", header=True)\n",
    "topics = se.read.csv(\"documents_topics.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 most visited document_ids in the page_views_sample log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "grouped_and_counted_views = views.groupBy(\"document_id\").count()\n",
    "first_10_rows = grouped_and_counted_views.orderBy(\"count\", ascending=False).limit(10)\n",
    "ans1 = first_10_rows.rdd.map(lambda row: int(row.document_id)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1811567,\n",
       " 234,\n",
       " 42744,\n",
       " 1858440,\n",
       " 1780813,\n",
       " 60164,\n",
       " 1790442,\n",
       " 1877626,\n",
       " 1821895,\n",
       " 732651]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many users have at least 2 different traffic_from pyspark.sql.functions import desc, countDistinctsources in the page_views_sample log (note the value is not a count, it's an encoded enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "grouped = views.groupBy(\"uuid\")\n",
    "aggregated_by_distinct = grouped.agg(distinct().count()countDistinct(\"traffic_source\").alias(\"dts\"))\n",
    "filtered = aggregatged_by_distinct.filter(aggregatged_by_distinct.dts >= 2)\n",
    "ans2 = filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98080"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 most visited topic_ids in page_views_sample log (use documents_topics table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined = views.join(documents_topics, on=\"document_id\", how=\"inner\")\n",
    "grouped_and_counted = joined.groupBy(\"topic_id\").count()\n",
    "sorted_and_limited = grouped_and_counted.orderBy(\"count\", ascending=False).limit(10)\n",
    "ans3 = sorted_and_limited.rdd.map(lambda row: int(row.topic_id)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 16, 216, 136, 140, 143, 36, 97, 8, 269]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = {\"top_10_documents\": ans1,\n",
    "          \"users\": ans2,\n",
    "          \"top_10_topics\": ans3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"answer.json\", \"w\") as f:\n",
    "    json.dump(answer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "! curl -F file=@answer.json \"51.250.54.133:80/MDS-LSML1/Vamiko/w4/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_seminar (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
